{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/14/19\n",
    "\n",
    "Okay, so today is the day that I start wrangling the medical dataset. First thing is first: get the catheter model working as a slicing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir('/lfs/1/gangus/repositories/pytorch-classification')\n",
    "\n",
    "import os.path as osp\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.font_manager as font_manager\n",
    "\n",
    "# import models.cifar as models\n",
    "# from analysis.analysis_utils import load_trained_model, fetch_dataloaders, get_coarse_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    'dataroot': True,\n",
    "    'batchSize': 1,\n",
    "    'loadSize': 512,\n",
    "    \n",
    "}     \n",
    "dataroot', required=True, help='path to images (should have subfolders trainA, trainB, valA, valB, etc)')\n",
    "batchSize', type=int, default=1, help='input batch size')\n",
    "loadSize', type=int, default=512, help='scale images to this size')\n",
    "fineSize', type=int, default=512, help='then crop to this size')\n",
    "input_nc', type=int, default=3, help='# of input image channels')\n",
    "output_nc', type=int, default=3, help='# of output image channels')\n",
    "ngf', type=int, default=64, help='# of gen filters in first conv layer')\n",
    "which_model_netG', type=str, default='srcnn', help='selects model to use for netG')\n",
    "gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n",
    "name', type=str, default='experiment_name', help='name of the experiment. It decides where to store samples and models')\n",
    "dataset_mode', type=str, default='alignedsrcnn', help='chooses how datasets are loaded.')\n",
    "model', type=str, default='srcnn',\n",
    "                     help='chooses which model to use')\n",
    "which_direction', type=str, default='AtoB', help='AtoB or BtoA')\n",
    "nThreads', default=2, type=int, help='# threads for loading data')\n",
    "checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n",
    "norm', type=str, default='instance', help='instance normalization or batch normalization')\n",
    "serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')\n",
    "display_winsize', type=int, default=256,  help='display window size')\n",
    "display_id', type=int, default=1, help='window id of the web display')\n",
    "display_port', type=int, default=8097, help='visdom port of the web display')\n",
    "no_dropout', action='store_true', help='no dropout for the generator')\n",
    "max_dataset_size', type=int, default=float(\"inf\"), help='Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.')\n",
    "resize_or_crop', type=str, default='none', help='scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop]')\n",
    "no_flip', action='store_true', help='if specified, do not flip the images for data augmentation')\n",
    "init_type', type=str, default='normal', help='network initialization [normal|xavier|kaiming|orthogonal]')\n",
    "\n",
    "self.parser.add_argument('--ntest', type=int, default=float(\"inf\"), help='# of test examples.')\n",
    "        self.parser.add_argument('--results_dir', type=str, default='./results/', help='saves results here.')\n",
    "        self.parser.add_argument('--aspect_ratio', type=float, default=1.0, help='aspect ratio of result images')\n",
    "        self.parser.add_argument('--phase', type=str, default='test', help='train, val, test, etc')\n",
    "        self.parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n",
    "        self.parser.add_argument('--how_many', type=int, default=100, help='how many test images to run')\n",
    "        self.parser.add_argument('--sourceoftest', type=str, default='internal', help='internal or external')\n",
    "\n",
    "        self.isTrain = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20 classes...\n",
      "Using coarse labels...\n",
      "Using coarse labels...\n"
     ]
    }
   ],
   "source": [
    "config = experiment_configs['baseline']\n",
    "\n",
    "superclass = config['superclass']\n",
    "cifar_type = config['cifar_type']\n",
    "model_name = config['model_name']\n",
    "model_args = config['model_args']\n",
    "checkpoint_dir = config['checkpoint_dir']\n",
    "trained_model = load_trained_model(cifar_type, model_name, model_args, checkpoint_dir)\n",
    "\n",
    "data_dir = config['data_dir']\n",
    "dataset_configs = config['dataset_configs']\n",
    "dataloader_configs = config['dataloader_configs']\n",
    "dataloaders = fetch_dataloaders(data_dir, cifar_type, superclass, dataset_configs, dataloader_configs)\n",
    "\n",
    "# Getting class-to-index maps for CIFAR100\n",
    "class_to_idx = {}\n",
    "class_to_idx['super'] = dataloaders['test'].dataset.class_to_idx\n",
    "class_to_idx['sub'] = dataloaders['test'].dataset.fine_class_to_idx\n",
    "\n",
    "idx_to_class = {}\n",
    "for tp in ['super', 'sub']:\n",
    "    idx_to_class[tp] = {v:k for k, v in class_to_idx[tp].items()}\n",
    "    \n",
    "trained_model = trained_model.to('cpu')\n",
    "trained_model = trained_model.module.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/100 [00:04<07:17,  4.41s/it]\u001b[A\n",
      "  2%|▏         | 2/100 [00:06<05:51,  3.58s/it]\u001b[A\n",
      "  3%|▎         | 3/100 [00:08<05:06,  3.16s/it]\u001b[A\n",
      "  4%|▍         | 4/100 [00:09<04:12,  2.63s/it]\u001b[A\n",
      "  5%|▌         | 5/100 [00:10<03:20,  2.11s/it]\u001b[A\n",
      "  6%|▌         | 6/100 [00:11<02:47,  1.78s/it]\u001b[A\n",
      "  7%|▋         | 7/100 [00:12<02:17,  1.48s/it]\u001b[A\n",
      "  8%|▊         | 8/100 [00:13<01:57,  1.28s/it]\u001b[A\n",
      "  9%|▉         | 9/100 [00:14<01:47,  1.18s/it]\u001b[A\n",
      " 10%|█         | 10/100 [00:15<01:40,  1.12s/it]\u001b[A\n",
      " 11%|█         | 11/100 [00:16<01:36,  1.08s/it]\u001b[A\n",
      " 12%|█▏        | 12/100 [00:17<01:49,  1.25s/it]\u001b[A\n",
      " 13%|█▎        | 13/100 [00:18<01:38,  1.13s/it]\u001b[A\n",
      " 14%|█▍        | 14/100 [00:19<01:29,  1.04s/it]\u001b[A\n",
      " 15%|█▌        | 15/100 [00:20<01:29,  1.05s/it]\u001b[A\n",
      " 16%|█▌        | 16/100 [00:21<01:23,  1.01it/s]\u001b[A\n",
      " 17%|█▋        | 17/100 [00:22<01:27,  1.06s/it]\u001b[A\n",
      " 18%|█▊        | 18/100 [00:23<01:24,  1.04s/it]\u001b[A\n",
      " 19%|█▉        | 19/100 [00:24<01:22,  1.02s/it]\u001b[A\n",
      " 20%|██        | 20/100 [00:25<01:29,  1.12s/it]\u001b[A\n",
      " 21%|██        | 21/100 [00:26<01:24,  1.07s/it]\u001b[A\n",
      " 22%|██▏       | 22/100 [00:27<01:19,  1.02s/it]\u001b[A\n",
      " 23%|██▎       | 23/100 [00:28<01:17,  1.00s/it]\u001b[A\n",
      " 24%|██▍       | 24/100 [00:29<01:12,  1.05it/s]\u001b[A\n",
      " 25%|██▌       | 25/100 [00:30<01:18,  1.04s/it]\u001b[A\n",
      " 26%|██▌       | 26/100 [00:31<01:18,  1.06s/it]\u001b[A\n",
      " 27%|██▋       | 27/100 [00:33<01:22,  1.13s/it]\u001b[A\n",
      " 28%|██▊       | 28/100 [00:34<01:27,  1.22s/it]\u001b[A\n",
      " 29%|██▉       | 29/100 [00:35<01:21,  1.14s/it]\u001b[A\n",
      " 30%|███       | 30/100 [00:36<01:14,  1.06s/it]\u001b[A\n",
      " 31%|███       | 31/100 [00:37<01:13,  1.07s/it]\u001b[A\n",
      " 32%|███▏      | 32/100 [00:38<01:09,  1.03s/it]\u001b[A\n",
      " 33%|███▎      | 33/100 [00:39<01:08,  1.02s/it]\u001b[A\n",
      " 34%|███▍      | 34/100 [00:40<01:09,  1.06s/it]\u001b[A\n",
      " 35%|███▌      | 35/100 [00:41<01:12,  1.12s/it]\u001b[A\n",
      " 36%|███▌      | 36/100 [00:42<01:12,  1.13s/it]\u001b[A\n",
      " 37%|███▋      | 37/100 [00:43<01:06,  1.06s/it]\u001b[A\n",
      " 38%|███▊      | 38/100 [00:44<01:03,  1.02s/it]\u001b[A\n",
      " 39%|███▉      | 39/100 [00:45<00:59,  1.02it/s]\u001b[A\n",
      " 40%|████      | 40/100 [00:46<00:58,  1.02it/s]\u001b[A\n",
      " 41%|████      | 41/100 [00:47<00:57,  1.03it/s]\u001b[A\n",
      " 42%|████▏     | 42/100 [00:48<00:57,  1.00it/s]\u001b[A\n",
      " 43%|████▎     | 43/100 [00:49<01:02,  1.10s/it]\u001b[A\n",
      " 44%|████▍     | 44/100 [00:51<01:04,  1.15s/it]\u001b[A\n",
      " 45%|████▌     | 45/100 [00:52<01:02,  1.13s/it]\u001b[A\n",
      " 46%|████▌     | 46/100 [00:53<00:57,  1.07s/it]\u001b[A\n",
      " 47%|████▋     | 47/100 [00:54<00:58,  1.11s/it]\u001b[A\n",
      " 48%|████▊     | 48/100 [00:55<00:58,  1.13s/it]\u001b[A\n",
      " 49%|████▉     | 49/100 [00:57<01:04,  1.26s/it]\u001b[A\n",
      " 50%|█████     | 50/100 [00:58<01:03,  1.27s/it]\u001b[A\n",
      " 51%|█████     | 51/100 [00:59<00:57,  1.18s/it]\u001b[A\n",
      " 52%|█████▏    | 52/100 [01:00<00:51,  1.08s/it]\u001b[A\n",
      " 53%|█████▎    | 53/100 [01:01<00:47,  1.01s/it]\u001b[A\n",
      " 54%|█████▍    | 54/100 [01:02<00:47,  1.03s/it]\u001b[A\n",
      " 55%|█████▌    | 55/100 [01:03<00:51,  1.14s/it]\u001b[A\n",
      " 56%|█████▌    | 56/100 [01:05<00:54,  1.24s/it]\u001b[A\n",
      " 57%|█████▋    | 57/100 [01:06<00:50,  1.18s/it]\u001b[A\n",
      " 58%|█████▊    | 58/100 [01:07<00:45,  1.09s/it]\u001b[A\n",
      " 59%|█████▉    | 59/100 [01:07<00:41,  1.00s/it]\u001b[A\n",
      " 60%|██████    | 60/100 [01:08<00:37,  1.07it/s]\u001b[A\n",
      " 61%|██████    | 61/100 [01:09<00:36,  1.07it/s]\u001b[A\n",
      " 62%|██████▏   | 62/100 [01:10<00:38,  1.01s/it]\u001b[A\n",
      " 63%|██████▎   | 63/100 [01:11<00:40,  1.08s/it]\u001b[A\n",
      " 64%|██████▍   | 64/100 [01:13<00:40,  1.13s/it]\u001b[A\n",
      " 65%|██████▌   | 65/100 [01:14<00:41,  1.18s/it]\u001b[A\n",
      " 66%|██████▌   | 66/100 [01:15<00:40,  1.18s/it]\u001b[A\n",
      " 67%|██████▋   | 67/100 [01:16<00:35,  1.09s/it]\u001b[A\n",
      " 68%|██████▊   | 68/100 [01:17<00:32,  1.01s/it]\u001b[A\n",
      " 69%|██████▉   | 69/100 [01:18<00:31,  1.00s/it]\u001b[A\n",
      " 70%|███████   | 70/100 [01:19<00:30,  1.03s/it]\u001b[A\n",
      " 71%|███████   | 71/100 [01:20<00:29,  1.03s/it]\u001b[A\n",
      " 72%|███████▏  | 72/100 [01:21<00:29,  1.04s/it]\u001b[A\n",
      " 73%|███████▎  | 73/100 [01:22<00:29,  1.09s/it]\u001b[A\n",
      " 74%|███████▍  | 74/100 [01:23<00:27,  1.07s/it]\u001b[A\n",
      " 75%|███████▌  | 75/100 [01:24<00:26,  1.05s/it]\u001b[A\n",
      " 76%|███████▌  | 76/100 [01:25<00:23,  1.02it/s]\u001b[A\n",
      " 77%|███████▋  | 77/100 [01:26<00:21,  1.07it/s]\u001b[A\n",
      " 78%|███████▊  | 78/100 [01:27<00:20,  1.09it/s]\u001b[A\n",
      " 79%|███████▉  | 79/100 [01:28<00:20,  1.05it/s]\u001b[A\n",
      " 80%|████████  | 80/100 [01:29<00:19,  1.05it/s]\u001b[A\n",
      " 81%|████████  | 81/100 [01:30<00:18,  1.02it/s]\u001b[A\n",
      " 82%|████████▏ | 82/100 [01:31<00:19,  1.06s/it]\u001b[A\n",
      " 83%|████████▎ | 83/100 [01:32<00:19,  1.13s/it]\u001b[A\n",
      " 84%|████████▍ | 84/100 [01:33<00:17,  1.07s/it]\u001b[A\n",
      " 85%|████████▌ | 85/100 [01:34<00:14,  1.01it/s]\u001b[A\n",
      " 86%|████████▌ | 86/100 [01:35<00:13,  1.01it/s]\u001b[A\n",
      " 87%|████████▋ | 87/100 [01:36<00:12,  1.01it/s]\u001b[A\n",
      " 88%|████████▊ | 88/100 [01:37<00:12,  1.06s/it]\u001b[A\n",
      " 89%|████████▉ | 89/100 [01:39<00:13,  1.20s/it]\u001b[A\n",
      " 90%|█████████ | 90/100 [01:40<00:12,  1.25s/it]\u001b[A\n",
      " 91%|█████████ | 91/100 [01:41<00:10,  1.15s/it]\u001b[A\n",
      " 92%|█████████▏| 92/100 [01:42<00:08,  1.04s/it]\u001b[A\n",
      " 93%|█████████▎| 93/100 [01:43<00:07,  1.08s/it]\u001b[A\n",
      " 94%|█████████▍| 94/100 [01:45<00:07,  1.22s/it]\u001b[A\n",
      " 95%|█████████▌| 95/100 [01:46<00:06,  1.34s/it]\u001b[A\n",
      " 96%|█████████▌| 96/100 [01:47<00:05,  1.28s/it]\u001b[A\n",
      " 97%|█████████▋| 97/100 [01:48<00:03,  1.21s/it]\u001b[A\n",
      " 98%|█████████▊| 98/100 [01:50<00:02,  1.17s/it]\u001b[A\n",
      " 99%|█████████▉| 99/100 [01:51<00:01,  1.17s/it]\u001b[A\n",
      "100%|██████████| 100/100 [01:52<00:00,  1.13s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "filenames_all = []\n",
    "coarse_labels_all = []\n",
    "fine_labels_all = []\n",
    "fine_preds_all = []\n",
    "preds_all = []\n",
    "losses_all = []\n",
    "softmax_all = []\n",
    "features_all = []\n",
    "\n",
    "t = tqdm(total=len(dataloaders['test']))\n",
    "for batch_idx, (inputs, \n",
    "                targets, \n",
    "                coarse_labels, \n",
    "                fine_labels, \n",
    "                filenames) in enumerate(dataloaders['test']):\n",
    "    \n",
    "    inputs, targets = inputs.cpu(), targets.cpu()\n",
    "    inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "    \n",
    "    outputs = trained_model(inputs)\n",
    "\n",
    "    _, preds = outputs.topk(1, 1, True, True)\n",
    "    soft = torch.nn.Softmax(dim=-1)(outputs)\n",
    "    # feats = extract_resnext_features(trained_model,inputs)\n",
    "    \n",
    "    coarse_labels_all+=list([int(a) for a in coarse_labels])\n",
    "    fine_labels_all+=list([int(a) for a in fine_labels])\n",
    "    preds_all+=list([int(a) for a in preds])\n",
    "    softmax_all+=list([a.detach().cpu().numpy() for a in soft])\n",
    "    # features_all+=list([a.detach().cpu().numpy() for a in feats])\n",
    "    filenames_all+=list(filenames)\n",
    "    t.update()\n",
    "t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dict = {\n",
    "    'filename':filenames_all,\n",
    "    'fine_labels':fine_labels_all,\n",
    "    'coarse_labels':coarse_labels_all,\n",
    "    'preds':preds_all,\n",
    "    'fine_labels_string': [idx_to_class['sub'][a] for a in fine_labels_all],\n",
    "    'coarse_labels_string': [idx_to_class['super'][a] for a in coarse_labels_all],\n",
    "    # 'features':features_all,\n",
    "    'softmax':softmax_all,\n",
    "}\n",
    "with open(f\"{checkpoint_dir}/predictions.pkl\",'wb') as f:\n",
    "   pickle.dump(preds_dict, f)\n",
    "\n",
    "prediction_df = pd.DataFrame(preds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_results = get_coarse_accuracies(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melting dataframe for plotting\n",
    "plot_df = pd.DataFrame(coarse_results).reset_index()\n",
    "plot_df = plot_df.rename(columns={\"index\": \"Superclass\"})\n",
    "plot_df = plot_df.melt(id_vars=['Superclass'], var_name='Subset', value_name='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2) \n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_style({'font.family':'serif'})\n",
    "\n",
    "font = font_manager.FontProperties(family='Dejavu Serif',\n",
    "                                   style='normal', size=16)\n",
    "plt.rcParams.update({'font.family': 'serif'})\n",
    "\n",
    "palette = [sns.xkcd_rgb[\"black\"]] + 5*[sns.xkcd_rgb[\"blue\"]]\n",
    "sns.set(font_scale=1.5)\n",
    "f = plt.figure(figsize=(10,10))\n",
    "ax = sns.stripplot(x=\"Superclass\", y=\"Accuracy\", hue=\"Subset\", data=plot_df, jitter=False, s=20, palette=palette, alpha=0.5)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=90,fontdict={'family':'Dejavu Serif'})\n",
    "# ax.set_yticklabels(ax.get_yticklabels(),fontdict={'family':'Dejavu Serif'})\n",
    "ax.set_xlabel(ax.get_xlabel(),fontdict={'family':'Dejavu Serif'})\n",
    "ax.set_ylabel(ax.get_ylabel(),fontdict={'family':'Dejavu Serif'})\n",
    "h,l = ax.get_legend_handles_labels()\n",
    "l[1] = 'Subclasses'\n",
    "l[0] = 'Superclass'\n",
    "ax.legend_.remove()\n",
    "ax.legend(h[0:2],l[0:2], ncol=1, loc='lower right',prop=font)\n",
    "ax.set_ylim([-0.02,1.02])\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{checkpoint_dir}/Superclass-Subclass-CIFAR-100-Correct-Val-v3.png',format='png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
